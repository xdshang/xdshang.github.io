<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="renderer" content="webkit">
        <title>VidOR Dataset</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="The dataset">
        <meta name="author" content="Xindi Shang, Donglin Di and Junbin Xiao">

        <link href="styles/bootstrap.min.css" rel="stylesheet">
        <link href="styles/common.css" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <h2 class="navbar-brand hidden-xs hidden-sm" >
                        <span class="logo">VidOR</span>
                        <span class="description">Video Object Relation Dataset</span>
                    </h2>
                    <h2 class="navbar-brand hidden-md hidden-lg" >
                        <span class="logo">VidOR</span>
                    </h2>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a class="page-scroll" href="#introduction">Introduction</a></li>
                    <li><a class="page-scroll" href="#downloads">Downloads</a></li>
                    <li><a class="page-scroll" href="#statistics">Statistics</a></li>
                    <li><a class="page-scroll" href="#citations">Citations</a></li>
                </ul>
                </div>
            </div>
        </nav>

        <section id="introduction" class="scrollable-section" style="padding-top: 50px;">
            <div class="container">
                <h3>Introduction</h3>
                <p>
                    VidOR (Video Object Relation) dataset contains 10,000 videos (98.6 hours) from 
                        <a href="https://multimediacommons.wordpress.com/yfcc100m-core-dataset/" target="_blank">YFCC100M collection</a>
                        together with a large amount of fine-grained annotations for relation understanding.
                    In particular, 80 categories of <a href="vidor/entity.html">objects</a>
                        are annotated with bounding-box trajectory to indicate their spatio-temporal location in the videos;
                        and 50 categories of relation <a href="vidor/predicate.html">predicates</a> are annotated
                        among all pairs of annotated objects with starting and ending frame index.
                    This results in around 50,000 object and 380,000 relation instances annotated.
                    To use the dataset for model development, the dataset is split into
                        7,000 videos for training, 835 videos for validation, and 2,165 videos for testing.
                    VidOR can provide foundation for many kinds of research and has been used in:
                </p>
                <ul>
                    <li><a href="https://nextplusplus.github.io/VidVRD-helper/mm19-gdc/index.html" target="_blank">ACM Multimedia 2019 Grand Challenge</a></li>
                    <li><a href="https://nextplusplus.github.io/VidVRD-helper/mm20-gdc/index.html" target="_blank">ACM Multimedia 2020 Grand Challenge</a></li>
                    <li><a href="https://videorelation.nextcenter.org/" target="_blank">ACM Multimedia 2021 Grand Challenge</a></li>
                    <li><a href="https://github.com/Guaranteer/VidSTG-Dataset" target="_blank">VidSTG</a>:
                        a dataset for video grounding</li>
                    <li><a href="https://doc-doc.github.io/docs/nextqa.html" target="_blank">NExT-QA</a>:
                        a video QA dataset for explaining temporal actions</li>
                    <li><a href="https://dl.acm.org/doi/10.1145/3474085.3475173" target="_blank">VidOR-MPVC</a>:
                        a video captioning dataset for multi-perspective visual captioning</li>
                </ul>
                <br>
                <div class="image-grid hidden-xs">
                    <img src="vidor/samples/3578746529.gif" alt="visualization-eg1" height="150" width="220">
                    <img src="vidor/samples/3909060617.gif" alt="visualization-eg2" height="150" width="220">
                    <img src="vidor/samples/4752448635.gif" alt="visualization-eg3" height="150" width="220">
                    <img src="vidor/samples/5100454414.gif" alt="visualization-eg4" height="150" width="220">
                    <img src="vidor/samples/5178231559.gif" alt="visualization-eg5" height="150" width="220">
                    <img src="vidor/samples/5571958942.gif" alt="visualization-eg6" height="150" width="220">
                    <img src="vidor/samples/6797818033.gif" alt="visualization-eg7" height="150" width="220">
                    <img src="vidor/samples/8540312536.gif" alt="visualization-eg8" height="150" width="220">
                    <img src="vidor/samples/9231052427.gif" alt="visualization-eg9" height="150" width="220">
                    <img src="vidor/samples/10148360995.gif" alt="visualization-eg10" height="150" width="220">
                </div>
                <div class="image-grid hidden-sm hidden-md hidden-lg">
                    <img src="vidor/samples/3578746529.gif" alt="visualization-eg1" height="150" width="220">
                    <img src="vidor/samples/8540312536.gif" alt="visualization-eg8" height="150" width="220">
                    <img src="vidor/samples/10148360995.gif" alt="visualization-eg10" height="150" width="220">
                </div>
            </div>
        </section>

        <section id="downloads" class="scrollable-section">
            <div class="container">
                <h3>Downloads</h3>
                <p>
                    Please download the videos in training/validation set using the following links, and extract video frames using
                    <a href="https://zdtnag7mmr.larksuite.com/file/boxusDps3gB5flan0xVrmMjOYBh">FFmpeg-3.3.4</a>.
                    The total sizes of training and validation videos is around 24.5G and 2.9G, respectively.
                    <em>It is recommended to unarchive the downloaded parts into the same directory.</em>
                    <br>
                    Alternatively, as the videos are exclusively drawn from the YFCC100M collection without any processing, you can also obtain
                    them via AWS S3 data storage hosted by
                    <a href="http://multimedia-commons.s3-website-us-west-2.amazonaws.com/" target="_blank">Multimedia Commons</a>
                    but need to organize the files in the directory structure consistent with the "<em>video_path</em>" in the annotations.
                    <ul>
                        <li>
                            training videos (part 1)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus2AXIPg8UNyFKBLOWR1sL0d">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1fIGQDd7j8JZMnG_kESAlTg?pwd=g2e2">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: eec7a718c05a16e388d6ee23102370d3]</small>
                        </li>
                        <li>
                            training videos (part 2)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxusvkEtuShImj7c5bC6Xj8wlf">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1U-6C9wxLi3ITaZxcuhfbSQ?pwd=7d97">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: 40ab82dccd9084c855452f4a2886137a]</small>
                        </li>
                        <li>
                            training videos (part 3)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus2Ctk4K9L4owUVn5LD0nZ0b">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1mwU9XGaPV5h53GKDoW9PZA?pwd=wqye">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: bdea6aa584913600fe5da95902fd938a]</small>
                        </li>
                        <li>
                            training videos (part 4)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus4kvvtlRaw1osI2Z3cNmiEf">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1Xjqx7HNR1R__8OyOO8Aayg?pwd=2u2n">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: a8dd511d36da4057a9b52f461065126f]</small>
                        </li>
                        <li>
                            training videos (part 5)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxusSnJZTvD3jkxz9eabOBmB5f">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1RG4hgfCe8CVKrmCrH0EPzQ?pwd=u3en">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: b2daedc2aa5d118e71946ea2614901df]</small>
                        </li>
                        <li>
                            training videos (part 6)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus65l03LJiYCIy4qGlGBkTih">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1nfAd1FHZEs0ZX3gL-IFx1A?pwd=ig2r">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: bfa77115ce69256063f22e59f87b90d2]</small>
                        </li>
                        <li>
                            training videos (part 7)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus2UAKpluQpviqiw89mbNqnW">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1tFjfw280_cxafzWkIUFs6w?pwd=9apb">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: b4ff9a77d2629d372115e4fe8a9398b4]</small>
                        </li>
                        <li>
                            training videos (part 8)
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxusAZxeowjuvzGMNvgQUCliUg">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1mdlq7cjPQdwKIwWv53E8mw?pwd=1bcc">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: e222802b0ce37aec1c3f14e81ea7e2c0]</small>
                        </li>
                        <li>
                            validation videos
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxusTAhmcL3wyq2frgufIT79Sg">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1wbc_MEJpXkeMFMhNV_MVDw?pwd=9qyu">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: b5386d83faeed98b8342ed31be737900]</small>
                        </li>
                        <li>
                            testing videos are available after participating the grand challenges
                        </li>
                    </ul>
                </p>
                <p>
                    Please download the annotations in training/validation set using the following links,
                    in which one JSON file contains the annotation for one video.
                    The format of a JSON file is shown in below, and you can load the annotations together using this helper 
                    <a href="https://github.com/xdshang/VidVRD-helper/blob/master/dataset/vidor.py" target="_blank">script</a>.
                    <ul>
                        <li>
                            training annotations
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxusYOED9ZqHMID9Pl5CwnFIxg">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1Mi1Y6jjFpjXGN_ADXKELVw?pwd=a2gz">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: 85f39fdd81a780bbb9b975cca8f219a2]</small>
                        </li>
                        <li>
                            validation annotations
                            <a href="https://zdtnag7mmr.larksuite.com/file/boxus7W409u5zgwbpA2DMIgq7yh">[Lark]</a>
                            <a href="https://pan.baidu.com/s/1dMZSI9CGnjHiZEKI4-GC8A?pwd=i6cm">[Baidu]</a>
                            <a href="https://huggingface.co/datasets/shangxd/vidor/tree/main">[HuggingFace]</a>
                            <small>[MD5: 96c6870910e8d4fb3836878215432c1f]</small>
                        </li>
                    </ul>
                </p>
<pre class="hidden-sm hidden-md hidden-lg">
<em class="comment">Please view the JSON format in larger screen</em>
</pre>
<pre class="hidden-xs">
{
    "version": "VERSION 1.0",
    "video_id": "5159741010",                       <em class="comment"># Video ID in YFCC100M collection</em>
    "video_hash": "6c7a58bb458b271f2d9b45de63f3a2", <em class="comment"># Video hash offically used for indexing in YFCC100M collection </em>
    "video_path": "1025/5159741010.mp4",            <em class="comment"># Relative path name in this dataset</em>
    "frame_count": 219,
    "fps": 29.97002997002997, 
    "width": 1920, 
    "height": 1080, 
    "subject/objects": [                            <em class="comment"># List of subject/objects</em>
        {
            "tid": 0,                               <em class="comment"># Trajectory ID of a subject/object</em>
            "category": "bicycle"
        }, 
        ...
    ], 
    "trajectories": [                               <em class="comment"># List of frames</em>
        [                                           <em class="comment"># List of bounding boxes in each frame</em>
            {                                       <em class="comment"># The bounding box at the 1st frame</em>
                "tid": 0,                           <em class="comment"># The trajectory ID to which the bounding box belongs</em>
                "bbox": {
                    "xmin": 672,                    <em class="comment"># Left</em>
                    "ymin": 560,                    <em class="comment"># Top</em>
                    "xmax": 781,                    <em class="comment"># Right</em>
                    "ymax": 693                     <em class="comment"># Bottom</em>
                }, 
                "generated": 0,                     <em class="comment"># 0 - the bounding box is manually labeled</em>
                                                    <em class="comment"># 1 - the bounding box is automatically generated by a tracker</em>
                "tracker": "none"                   <em class="comment"># If generated=1, it is one of "linear", "kcf" and "mosse"</em>
            }, 
            ...
        ],
        ...
    ],
    "relation_instances": [                         <em class="comment"># List of annotated visual relation instances</em>
        {
            "subject_tid": 0,                       <em class="comment"># Corresponding trajectory ID of the subject</em>
            "object_tid": 1,                        <em class="comment"># Corresponding trajectory ID of the object</em>
            "predicate": "in_front_of", 
            "begin_fid": 0,                         <em class="comment"># Frame index where this relation begins (inclusive)</em>
            "end_fid": 210                          <em class="comment"># Frame index where this relation ends (exclusive)</em>
        }, 
        ...
    ]
}
</pre>
            </div>
        </section>

        <section id="statistics" class="scrollable-section">
            <div class="container">
                <h3>Statistics</h3>
                <p>
                    This section provides an overview of the dataset statistics. A detailed description about the
                    dataset can be found in this <a href="https://dl.acm.org/authorize?N680489">paper</a>.
                </p>
                <div class="container">
                    <figure class="figure" style="margin-bottom: 12px;">
                        <figcaption>
                            Figure 1: Statistics for video lengths in train/val set.
                            The lengths of most videos are distributed between 3 and 93 seconds.
                            The shortest video is 1.00 second, and the longest video of them is 180.01 seconds.
                            Overall, the average video length in train/val set is 35.73 seconds.
                        </figcaption>
                        <br>
                        <img src="vidor/train_val_vid_length.png" class="figure-img img-responsive center-block" alt="train_val_vid_length">
                    </figure>
                </div>
                <br>
                <div class="container">
                    <figure class="figure" style="margin-bottom: 12px;">
                        <figcaption>
                            Figure 2: Objects statistics per category in train/val set.
                            The categories are grouped into three upper level categories: Human(3), Animal(28) and Other(49).
                            It can be found that the number of objects in Human accounts for 56.34% of the proportion,
                                while the portion of objects in Animal and Other are 35.78% and 7.98%, respectively.
                        </figcaption>
                        <br>
                        <img src="vidor/train_val_object.png" class="figure-img img-responsive center-block" alt="train_val_object">
                    </figure>
                </div>
                <br>
                <div class="container">
                    <figure class="figure" style="margin-bottom: 12px;">
                        <figcaption>
                            Figure 3: Predicate statistics per category in train/val set.
                            Each bar indicates the number of relation instances whose predicate belongs to that category.
                            The two types of predicates (i.e. spatial(8) and actions(42)) are highlighted in different colors.
                            And their proportions are 76.77% and 23.23%, respectively.
                        </figcaption>
                        <br>
                        <img src="vidor/train_val_predicate.png" class="figure-img img-responsive center-block" alt="train_val_predicate">
                    </figure>
                </div>
                <br>
                <div class="container">
                    <figure class="figure" style="margin-bottom: 12px;">
                        <figcaption>
                            Figure 4: Statistics of the types of relation triplets in train/val set. 
                            The dark area shows the number and portion of triplet types unique in the training set;
                            the grey area shows that of triplet types appearing in both of the training and validation set;
                            and the light area shows that of triplet types unique in the validation set.
                            In the validation set, 641 out of 30,142 relation instances belong to the 295 triplet types (light area).
                        </figcaption>
                        <br>
                        <img src="vidor/train_val_triplet.png" class="figure-img img-responsive center-block">
                    </figure>
                </div>
            </div>
        </section>

        <section id="citations" class="scrollable-section">
            <div class="container">
                <h3>Citations</h3>
                <p>
                    Please kindly cite these works if the dataset helps your research.
                </p>
<pre>
@inproceedings{shang2019annotating,
    title={Annotating Objects and Relations in User-Generated Videos},
    author={Shang, Xindi and Di, Donglin and Xiao, Junbin and Cao, Yu and Yang, Xun and Chua, Tat-Seng},
    booktitle={Proceedings of the 2019 on International Conference on Multimedia Retrieval},
    pages={279--287},
    year={2019},
    organization={ACM}
}

@article{thomee2016yfcc100m,
    title={YFCC100M: The New Data in Multimedia Research},
    author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
    journal={Communications of the ACM},
    volume={59},
    number={2},
    pages={64--73},
    year={2016},
    publisher={ACM New York, NY, USA}
}
</pre>
            </div>
        </section>

        <!-- <footer>
            <div class="container">
                <hr>
                <p>&copy;</p>
            </div>
        </footer> -->

        <script type="text/javascript" src="scripts/jquery-3.2.1.min.js"></script>
        <script type="text/javascript" src="scripts/bootstrap.min.js"></script>
        <script type="text/javascript" src="scripts/jquery.easing.min.js"></script>
        <script type="text/javascript" src="scripts/scrolling-nav.js"></script>
    </body>
</html>
