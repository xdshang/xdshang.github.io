<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="renderer" content="webkit">
        <title>Task 1 - Video Object Detection - ACM MM'19 Grand Challenge</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="ACM Multimedia'19 Grand Challenge">
        <meta name="author" content="Junbin Xiao">

        <link href="../../styles/bootstrap.min.css" rel="stylesheet">
        <link href="../../styles/common.css" rel="stylesheet">
        <link href="../../styles/show-json.css" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <h2 class="navbar-brand hidden-xs hidden-sm" style="padding-top: 0px;padding-bottom: 0px;height: 30px;line-height: 35px">
                        <span class="logo">Relation Understanding in Videos</span>
                    </h2>
                    <h5 class="hidden-xs hidden-sm" style="margin-bottom: 0px">
                        <span style="color:#003d7c !important;">ACM Multimedia 2019 Grand Challenge</span>
                    </h5>
                    <h2 class="navbar-brand hidden-md hidden-lg">
                        <span class="logo">Relation Challenge</span>
                    </h2>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a class="page-scroll" href="index.html#task">Tasks</a></li>
                        <li><a class="page-scroll" href="index.html#timeline">Timeline</a></li>
                        <li><a class="page-scroll" href="index.html#leaderboard">Leaderboard</a></li>
                        <li><a class="page-scroll" href="index.html#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- linked this part-->
        <section id="introduction" class="scrollable-section">
            <div class="container">
                <h3>Task 1: Video Object Detection</h3>
                <p>
                    Object detection is the first step towards relation understanding in videos.
                    The main goal of this task is to develop robust object detectors
                        that not only localize objects from the 80 categories with bounding boxes in every video frame
                        but also link the bounding boxes that indicate same object entity into a trajectory.
                    The task requires participants to develop real video object detectors
                        that can understand the identities and dynamics of object entities at video level,
                        which can benefit many applications that require fine-grained video understanding.
                    The challenge will accelerate the research on robust object detection in videos in the wild,
                        by providing a larger number of user generated videos with annotations.
                    Additionally, the challenge will encourage the research on video object detection with relationships by providing relation 
                    annotation in the dataset. An expectable challenge in this task is that, the detectors should 
                    be able to re-identify objects which appear again after long-term disappearance. Technically, the detectors have to overcome
                    difficulties from two aspects:
                    <ul>
                        <li>video quality like free camera motion, illumination, and blurring.</li>
                        <li>object variation, like occlusion, and deformable shape.</li>
                    </ul>
                    <strong>In case any problems, feel free to contact</strong>: xiaojunbin@u.nus.edu
                </p>
                

                <h3>Dataset</h3>
                <p>
                    The dataset for this task consists of 10K user-generated videos from Flickr, 
                    along with annotations on 80 categories of object (e.g., "adult", "child", "dog", "table"). Bounding boxes are annotated
                    for objects in each frame, and the objects' identities among frames are also provided. The training/validation/testing splits
                    are 7,000, 835 and 2,165 videos. Specially, if the object just shows part of its body in the image (e.g., a hand of a person),
                    it is also annotated in this dataset. For more detail information, please go to the 
                    <a href="../../vidor.html" target="_blank">dataset</a> page.
                </p>
                <p>
                    The videos and annotations can be downloaded directly from <a href="../../vidor.html#downloads" target="_blank">here</a>.
                    Please note that the downloaded annotations contain additional annotation of relations.
                    This task allows participants to use them to train the object detection models.
                </p>


                <h3>Evaluation Metric</h3>
                <p>
                    We adopt average precision (AP) as metric to evaluate the detection performance for each category. The trajectory-level 
                    mean AP (mAP) is defined as follows:
                    <br>
                    Given a predicted trajectory (a.k.a. tubelet) $\mathcal{T}_p$ and a ground truth 
                    trajectory $\mathcal{T}_g$ of a certain category, the temporal Intersection over Union (tIoU) between these two trajectories is defined by:
                        $$\text{tIoU}(\mathcal{T}_p, \mathcal{T}_g)=\frac{D_p \cap D_g}{D_p\cup D_g}, $$
                    where $D_p$, $D_g$ denote the time duration of the predicted trajectory and the ground truth trajectory respectively. In out settings, the threshold for \( \text{tIoU} \) is 0.5,
                    which means any result with \( \text{tIoU} \geq 0.5 \) will be regarded as true positive prediction. Besides, the \( \text{IoU} \) threshold for frame-level bounding box is set to (0.5, 0.7, 0.9). For each trajectory pair,
                    their \( \text{tIoU} \) is averaged on the three frame-level IoU values. The final mAP is obtained by averaging the APs across all categories. Note that we will not independently evaluate the image-level detection performance in this challenge.
                </p>
                <p>
                    The evaluation code used by the evaluation server can be found
                    <a href="https://github.com/xdshang/VidVRD-helper/blob/master/evaluation/video_object_detection.py" target="_blank">here</a>.
                    The number of predictions per video is limited up to 200.
                </p>
            </div>
        </section>
        <section id="dataset" class="scrollable-section">
            <div class="container">
                <h3>Submission Format</h3>
                <p>
                    Please use the following JSON format when submitting your results for the challenge:
                </p>
                <pre id="sample"></pre>
                <p>
                    The example above is illustrative. Comments must be removed in your submission.
                    A sample submission file is available <a href="submission-task1.json">here</a>.
                </p>
            </div>
        </section>

        <footer class="footer" style="padding-top: 50px;">
            <div class="container">
                <hr>
                <p style="font-style: italic; text-align: center">
                    &copy; 2019 Organization Team:
                    Xindi Shang, Donglin Di, Junbin Xiao and 
                    <a href="https://www.chuatatseng.com" target="_blank">Tat-Seng Chua</a>
                    (National University of Singapore)
                </p>
            </div>
        </footer>

        <script type="text/javascript" src="../../scripts/jquery-3.2.1.min.js"></script>
        <script type="text/javascript" src="../../scripts/bootstrap.min.js"></script>
        <script type="text/javascript" src="../../scripts/jquery.easing.min.js"></script>
        <script type="text/javascript" src="../../scripts/scrolling-nav.js"></script>
        <script type="text/javascript" src="../../scripts/show-json.js"></script>
        <script type="text/javascript">
            const sample = "{\n" +
            "  version: \"VERSION 1.0\",\n" +
            "  results: {\n" +
            "    \"8173050335\": [                             # video ID\n" +
            "      {                                         # a trajectory instance\n" +
            "        category: \"dog\",                        # trajectory category\n" +
            "        score: 0.8943,                          # trajectory score float \n" +
            "        trajectory: {                           # trajectory\n" +
            "          \"5\": [9, 10, 45, 20],                 # frame_id: [x_min, y_min, x_max, y_max] int (all inclusive) \n" +
            "          \"6\": [10, 17, 46, 22],                   \n" +
            "          \"17\": [11, 12, 40, 25],\n" +
            "          \"19\": [13, 12, 43, 27]\n" +
            "        },\n" +
            "      },\n" +
            "      {                                         # another trajectory instance\n" +
            "        category: \"adult\", \n" +
            "        score: 0.6626,\n" +
            "        trajectory: { \n" +
            "         \"0\": [89, 10, 128, 80], \n" +
            "         \"1\": [90, 10, 128, 79]\n" +
            "        },\n" +
            "      }\n" +
            "    ]\n" +
            "  }\n" +
            "  external_data: {\n" +
            "    used: true,                                 # Boolean flag. True indicates the use of external data.\n" +
            "    details: \"First fully-connected layer from VGG-16 pre-trained on ILSVRC-2012 training set\" # This string details what kind of external data you used and how you used it.\n" +
            "  }\n" +
            "}";
            $('#sample').html(syntaxHighlight(sample));
        </script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>
        <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        
    </body>
</html>
